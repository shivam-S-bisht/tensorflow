{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf.text.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYOQJ0OmRCAO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "outputId": "edb5b145-b1ef-4760-8180-265949a19615"
      },
      "source": [
        "!pip install tensorflow-text"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-text\n",
            "  Using cached https://files.pythonhosted.org/packages/50/35/b87b84648f84b9021df3407422d3cf9afdcbb9b087d2bf293fe72557b7a5/tensorflow_text-2.1.1-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting tensorflow<2.2,>=2.1.0\n",
            "  Using cached https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1.0->tensorflow-text) (1.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1.0->tensorflow-text) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1.0->tensorflow-text) (0.2.0)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "  Using cached https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1.0->tensorflow-text) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1.0->tensorflow-text) (0.34.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1.0->tensorflow-text) (3.2.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1.0->tensorflow-text) (1.1.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1.0->tensorflow-text) (1.4.1)\n",
            "Processing /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd/gast-0.2.2-cp36-none-any.whl\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1.0->tensorflow-text) (1.28.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1.0->tensorflow-text) (3.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1.0->tensorflow-text) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1.0->tensorflow-text) (2.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1.0->tensorflow-text) (1.18.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1.0->tensorflow-text) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1.0->tensorflow-text) (0.9.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=2.1.0->tensorflow-text) (1.7.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=2.1.0->tensorflow-text) (46.1.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=2.1.0->tensorflow-text) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=2.1.0->tensorflow-text) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=2.1.0->tensorflow-text) (3.2.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=2.1.0->tensorflow-text) (2.23.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow<2.2,>=2.1.0->tensorflow-text) (2.10.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=2.1.0->tensorflow-text) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=2.1.0->tensorflow-text) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=2.1.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=2.1.0->tensorflow-text) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=2.1.0->tensorflow-text) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=2.1.0->tensorflow-text) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=2.1.0->tensorflow-text) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=2.1.0->tensorflow-text) (2020.4.5.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=2.1.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow<2.2,>=2.1.0->tensorflow-text) (3.1.0)\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0rc0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, gast, tensorflow, tensorflow-text\n",
            "  Found existing installation: tensorboard 2.2.1\n",
            "    Uninstalling tensorboard-2.2.1:\n",
            "      Successfully uninstalled tensorboard-2.2.1\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow 2.2.0rc4\n",
            "    Uninstalling tensorflow-2.2.0rc4:\n",
            "      Successfully uninstalled tensorflow-2.2.0rc4\n",
            "Successfully installed gast-0.2.2 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-text-2.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax-GHaMURwuq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text as text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgjpbM1wR7u_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "0295abee-68a9-4b10-cc10-a5feaa2a9c02"
      },
      "source": [
        "docs = tf.constant([u'Everything not saved will be lost.'.encode('UTF-16-BE'), u'Sad☹'.encode('UTF-16-BE')])\n",
        "utf8_docs = tf.strings.unicode_transcode(docs, input_encoding = 'UTF-16-BE', output_encoding = 'UTF-8')\n",
        "utf8_docs"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=string, numpy=\n",
              "array([b'Everything not saved will be lost.', b'Sad\\xe2\\x98\\xb9'],\n",
              "      dtype=object)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV8q2Hp_S6Rl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenize and split the sting with white spaces"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QV-XzAXTFlw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "98cb6a7d-06f5-456d-ddfd-7bd5850f495b"
      },
      "source": [
        "tokenizer = text.WhitespaceTokenizer()\n",
        "tokens = tokenizer.tokenize(utf8_docs)\n",
        "print(tokens.to_list())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/dispatch.py:180: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
            "Instructions for updating:\n",
            "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n",
            "[[b'Everything', b'not', b'saved', b'will', b'be', b'lost.'], [b'Sad\\xe2\\x98\\xb9']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkppbEbsThJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenize based on the script boundries"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_IBaLZxT36H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "eb2a47c9-8281-41b5-fce7-56ec903c7ab1"
      },
      "source": [
        "tokenizer = text.UnicodeScriptTokenizer()\n",
        "tokens = tokenizer.tokenize(utf8_docs)\n",
        "tokens.to_list()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[b'Everything', b'not', b'saved', b'will', b'be', b'lost', b'.'],\n",
              " [b'Sad', b'\\xe2\\x98\\xb9']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzjPHmiBUFzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for languages like chinese there are no white spaces\n",
        "# its better to split the sting char by char"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEODGPaEUZ7h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "938b3b28-efd2-41e8-98ef-0c675c2f4fef"
      },
      "source": [
        "tokens = tf.strings.unicode_split([u\"仅今年前\".encode('UTF-8')], input_encoding = 'UTF-8')\n",
        "tokens.to_list()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[b'\\xe4\\xbb\\x85', b'\\xe4\\xbb\\x8a', b'\\xe5\\xb9\\xb4', b'\\xe5\\x89\\x8d']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbptrUYyUqix",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "f2ae0dda-78c8-4a7f-f047-e2dd12121fd6"
      },
      "source": [
        "tokenizer = text.UnicodeScriptTokenizer()\n",
        "(tokens, offset_starts, offset_limits) = tokenizer.tokenize_with_offsets(utf8_docs)\n",
        "print(tokens.to_list())\n",
        "print(offset_starts.to_list())\n",
        "print(offset_limits.to_list())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[b'Everything', b'not', b'saved', b'will', b'be', b'lost', b'.'], [b'Sad', b'\\xe2\\x98\\xb9']]\n",
            "[[0, 11, 15, 21, 26, 29, 33], [0, 3]]\n",
            "[[10, 14, 20, 25, 28, 33, 34], [3, 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Htt1mHEkVm83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# they work same as tf.data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXz5OvYQV1lR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "26c571e3-842a-42db-93c1-e78b538c127a"
      },
      "source": [
        "docs = tf.data.Dataset.from_tensor_slices([['Never tell me the odds.'], [\"It's a trap!\"]])\n",
        "tokenizer = text.WhitespaceTokenizer()\n",
        "for i in docs: \n",
        "    print(i)\n",
        "tokenized_docs = docs.map(lambda x: tokenizer.tokenize(x))\n",
        "print(next(iter(tokenized_docs)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([b'Never tell me the odds.'], shape=(1,), dtype=string)\n",
            "tf.Tensor([b\"It's a trap!\"], shape=(1,), dtype=string)\n",
            "<tf.RaggedTensor [[b'Never', b'tell', b'me', b'the', b'odds.']]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HexdyFFDWkca",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "a4243595-b3cc-4308-9945-fa8387b30be4"
      },
      "source": [
        "tokenizer = text.WhitespaceTokenizer()\n",
        "tokens = tokenizer.tokenize(['Everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
        "\n",
        "# Is capitalized?\n",
        "f1 = text.wordshape(tokens, text.WordShape.HAS_TITLE_CASE)\n",
        "# Are all letters uppercased?\n",
        "f2 = text.wordshape(tokens, text.WordShape.IS_UPPERCASE)\n",
        "# Does the token contain punctuation?\n",
        "f3 = text.wordshape(tokens, text.WordShape.HAS_SOME_PUNCT_OR_SYMBOL)\n",
        "# Is the token a number?\n",
        "f4 = text.wordshape(tokens, text.WordShape.IS_NUMERIC_VALUE)\n",
        "\n",
        "print(f1.to_list())\n",
        "print(f2.to_list())\n",
        "print(f3.to_list())\n",
        "print(f4.to_list())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[True, False, False, False, False, False], [True]]\n",
            "[[False, False, False, False, False, False], [False]]\n",
            "[[False, False, False, False, False, True], [True]]\n",
            "[[False, False, False, False, False, False], [False]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egabamGMXB5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ngrams can be applied to the tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnG3kMU6XRNK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c6e35eb9-d47e-4946-da41-f6c11a61233a"
      },
      "source": [
        "tokenizer = text.WhitespaceTokenizer()\n",
        "tokens = tokenizer.tokenize(['Everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
        "\n",
        "# Ngrams, in this case bi-gram (n = 2)\n",
        "bigrams = text.ngrams(tokens, 2, reduction_type=text.Reduction.STRING_JOIN)\n",
        "\n",
        "print(bigrams.to_list())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[b'Everything not', b'not saved', b'saved will', b'will be', b'be lost.'], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1GhAwYFXlHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}